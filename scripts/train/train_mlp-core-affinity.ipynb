{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPL Training for core affinity\n",
    "\n",
    "This notebook goes over the steps to train the MLP network when the POC is not pinned to one specific core. To accommodate this, we train two different models. The first predicts whether the POC is running on the core for which we optimized our probe position, whereas the second model will predict the actual encoded bit. Note that in our case, we optimized the probe position for both cores 0 and 1.\n",
    "\n",
    "Besides training two models, the remaining parameters stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'   # Disable tensorflow logs\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.fft import fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and methods definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras generator\n",
    "\n",
    "The traces may not all fit into memory, which is why we need a generator.\n",
    "\n",
    "Adapted from https://github.com/angulartist/Keras-HDF5-ImageDataGenerator  \n",
    "Copyright (c) 2023, Jesse De Meulemeester  \n",
    "Copyright (c) 2017, HDF5 ImageDataGenerator All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Generator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Just a simple custom Keras HDF5 Generator.\n",
    "    \n",
    "    Custom Keras ImageDataGenerator that generates\n",
    "    batches of traces from HDF5 files\n",
    "     \n",
    "    Arguments\n",
    "    ---------\n",
    "    src : str\n",
    "        Path of the hdf5 source file.\n",
    "    x_key : str\n",
    "        Key of the h5 file image tensors dataset.\n",
    "    y_key : str\n",
    "        Key of the h5 file labels dataset.\n",
    "    classes_key : str\n",
    "        Key of the h5 file dataset containing\n",
    "        the raw classes.\n",
    "    batch_size : int\n",
    "        Size of each batch, must be a power of two.\n",
    "        (16, 32, 64, 128, 256, ...)\n",
    "        Default is 32.\n",
    "    shuffle : bool\n",
    "        Shuffle images at the end of each epoch.\n",
    "        Default is True.\n",
    "    indices : np.ndarray\n",
    "        Indices of the traces to use. None will use all available traces.\n",
    "        Default is None.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        f: h5py.File,\n",
    "        x_key: str,\n",
    "        y_key: str,\n",
    "        classes_key: str,\n",
    "        batch_size: int = 32,\n",
    "        shuffle: bool = True,\n",
    "        indices: np.ndarray = None\n",
    "    ):\n",
    "        self.f: h5py.File = f\n",
    "        self.x_key: str = x_key\n",
    "        self.y_key: str = y_key\n",
    "        self.classes_key: str = classes_key\n",
    "        self.batch_size: int = batch_size\n",
    "        self.shuffle: bool = shuffle\n",
    "\n",
    "        if indices is None:\n",
    "            self._indices = np.arange(self.__get_dataset_shape(self.x_key, 0))\n",
    "        else:\n",
    "            self._indices = indices\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Representation of the class.\"\"\"\n",
    "        return f\"{self.__class__.__name__}({self.__dict__!r})\"\n",
    "\n",
    "    def __get_dataset_shape(self, dataset: str, index: int):\n",
    "        \"\"\"Get an h5py dataset shape.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        dataset : str\n",
    "            The dataset key.\n",
    "        index : int\n",
    "            The dataset index.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of ints\n",
    "            A tuple of array dimensions.\n",
    "        \"\"\"\n",
    "        return self.f[self.classes_key][dataset].shape[index]\n",
    "\n",
    "    def __get_dataset_items(\n",
    "        self,\n",
    "        indices: np.ndarray,\n",
    "    ):\n",
    "        \"\"\"Get an HDF5 dataset items.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        indices : ndarray, \n",
    "            The list of current batch indices.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        a tuple of ndarrays\n",
    "            A batch of samples.\n",
    "        \"\"\"\n",
    "        return (self.f[self.classes_key][self.x_key][indices], self.f[self.classes_key][self.y_key][indices])\n",
    "    \n",
    "    @property\n",
    "    def num_items(self):\n",
    "        \"\"\"Grab the total number of examples\n",
    "         from the dataset.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The total number of examples.\n",
    "        \"\"\"\n",
    "        self.f[self.classes_key][self.x_key].shape[0]\n",
    "    \n",
    "    @property \n",
    "    def classes(self):\n",
    "        \"\"\"Grab \"human\" classes from the dataset.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list of the raw classes.\n",
    "        \"\"\"      \n",
    "        return self.f[self.classes_key][:]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch.\n",
    "         \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The number of batches per epochs.\n",
    "        \"\"\"\n",
    "        return len(self._indices) // self.batch_size\n",
    "\n",
    "    def __next_batch(self,\n",
    "                     indices: np.ndarray):\n",
    "        \"\"\"Generates a batch of train/val data for the given indices.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        index : int\n",
    "            The index for the batch.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of ndarrays\n",
    "            A tuple containing a batch of image tensors\n",
    "            and their associated labels.\n",
    "        \"\"\"\n",
    "        # Grab samples (tensors, labels) HDF5 source file.\n",
    "        return self.__get_dataset_items(indices)\n",
    "\n",
    "    def __getitem__(\n",
    "            self,\n",
    "            index: int):\n",
    "        \"\"\"Generates a batch of data for the given index.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        index : int\n",
    "            The index for the current batch.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple of ndarrays or ndarray\n",
    "            A tuple containing a batch of image tensors\n",
    "            and their associated labels (train) or\n",
    "            a tuple of image tensors (predict).\n",
    "        \"\"\"\n",
    "        # Indices for the current batch.\n",
    "        indices = np.sort(self._indices[index * self.batch_size:(index + 1) *\n",
    "                                        self.batch_size])\n",
    "\n",
    "        return self.__next_batch(indices)\n",
    "\n",
    "    def __shuffle_indices(self):\n",
    "        \"\"\"If the shuffle parameter is set to True,\n",
    "         dataset will be shuffled (in-place).\n",
    "        \"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self._indices)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Triggered once at the very beginning as well as \n",
    "         at the end of each epoch.\n",
    "        \"\"\"\n",
    "        self.__shuffle_indices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training and evaluation datasets\n",
    "\n",
    "We now transform the input traces into the actual parts we will use. For each trace, we keep the 1000 lowerest frequency components when taking the FFT over the optimal window to predict the bit, and 2000 lowest frequency components for the FFT over the whole trace to detect the core. We store these components for each trace in a new file which we will use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory containing all training data.\n",
    "# Note that in this case, we collected data on the four different cores.\n",
    "# The data from different cores will now be used to train the MLP models.\n",
    "directory_data = \"../traces/4-mlp-data/2-reducing-assumptions/1-core-affinity/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random string used in the POC SpectrEM implementations\n",
    "data = b\"data|\\x01\\x36\\x9b\\x78\\xc9\\x2c\\x3d\\x32\\xfa\\x83\\x50\\xaf\\x39\\xaf\\x69\\x2d\\x58\\xd7\\x38\\x6a\\xc1\\x63\\x15\\xc7\\x3c\\x4d\\x96\\x61\\xe1\\x88\\xbd\\xed\"\n",
    "data = np.frombuffer(data, dtype=np.uint8)\n",
    "def get_bit_sp(index):\n",
    "    \"\"\"Get the bit corresponding to the given inputs\n",
    "    \n",
    "    @param index The index for which to get the corresponding bit\"\"\"\n",
    "    return (data[index // 8] & (1 << (index % 8))).astype(bool)\n",
    "\n",
    "def get_bit_md(inputs):\n",
    "    \"\"\"Get the bit corresponding to the given inputs for MeltEMdown\n",
    "    \n",
    "    @param inputs The inputs for which to retrieve the secret bit\"\"\"\n",
    "    return inputs[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the datasets to detect the core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in (\"training-data\", \"validation-data\"):\n",
    "    with h5py.File(f\"{directory_data}/mlp-detectcore-{data}.hdf5\", 'x', libver='latest') as f:\n",
    "        expg = f.create_group(\"data\")\n",
    "        freqsDset = expg.create_dataset(\"traces\", (len(files)*4096*4, 2000), dtype='float32')\n",
    "        expectedDset = expg.create_dataset(\"expected\", (len(files)*4096*4, 2), dtype='uint8')\n",
    "        traces_i = 0\n",
    "\n",
    "        for core in range(4):\n",
    "            dir = f\"{directory_data}/core-{core}/{data}/\"\n",
    "            files = [f for f in os.listdir(dir) if f.endswith(\".hdf5\")][:4]  # 4 batches for each core\n",
    "\n",
    "            for file in tqdm(files):\n",
    "                with h5py.File(f\"{dir}/file\", 'r', libver='latest') as f_traces:\n",
    "                    traces = f_traces['data']['traces']\n",
    "                    inputs = f_traces['data']['inputs']\n",
    "\n",
    "                    for i in range(traces.shape[0]):\n",
    "                        # Note: instead of considering a specific window, this MLP network will take\n",
    "                        # as inputs the FFT over the entire trace (the first 2000 components).\n",
    "                        freqsDset[traces_i,:] = np.abs(fft(traces[i,:]))[:2000]\n",
    "\n",
    "                        if core <= 1:\n",
    "                            # We are interested in the traces where the POC is running on either core\n",
    "                            # 0 or core 1, as we optimized the probe positions for these cores.\n",
    "                            expectedDset[traces_i, 1] = 1\n",
    "                        else:\n",
    "                            expectedDset[traces_i, 0] = 1\n",
    "\n",
    "                        traces_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the datasets to predict the bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in (\"training-data\", \"validation-data\"):\n",
    "    with h5py.File(f\"{directory_data}/mlp-predictbit-{data}.hdf5\", 'x', libver='latest') as f:\n",
    "        expg = f.create_group(\"data\")\n",
    "        freqsDset = expg.create_dataset(\"traces\", (len(files)*4096*4, 1000), dtype='float32')\n",
    "        expectedDset = expg.create_dataset(\"expected\", (len(files)*4096*4, 2), dtype='uint8')\n",
    "        traces_i = 0\n",
    "\n",
    "        for core in range(2):  # To predict the bit, we only need to consider cores 0 and 1\n",
    "            dir = f\"{directory_data}/core-{core}/{data}/\"\n",
    "            files = [f for f in os.listdir(dir) if f.endswith(\".hdf5\")]\n",
    "\n",
    "            for file in tqdm(files):\n",
    "                with h5py.File(f\"{dir}/file\", 'r', libver='latest') as f_traces:\n",
    "                    traces = f_traces['data']['traces']\n",
    "                    inputs = f_traces['data']['inputs']\n",
    "\n",
    "                    window_start = f_traces.attrs[\"optimal_window_start\"]\n",
    "                    window = slice(window_start, window_start+5000)\n",
    "\n",
    "                    for i in range(traces.shape[0]):\n",
    "\n",
    "                        freqsDset[traces_i,:] = np.abs(fft(traces[i,window]))[:1000]\n",
    "                        expectedDset[traces_i, get_bit_sp(i)] = 1\n",
    "\n",
    "                        traces_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network -- detect core\n",
    "\n",
    "We now define the MLP network that we will use for evaluation to detect the core on which the POC is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_train = f\"{directory_data}/mlp-detectcore-training-data.hdf5\"\n",
    "fname_val = f\"{directory_data}/mlp-detectcore-validation-data.hdf5\"\n",
    "    \n",
    "f_train = h5py.File(fname_train, 'r', libver='latest')\n",
    "f_val = h5py.File(fname_val, 'r', libver='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = f_train[\"data\"][\"traces\"].shape[0]\n",
    "N_VAL = f_val[\"data\"][\"traces\"].shape[0]\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "VALIDATION_STEPS = N_VAL // BATCH_SIZE\n",
    "\n",
    "print(f\"Summary:\")\n",
    "print(f\"  Total number of traces: {N_TRAIN + N_VAL}\")\n",
    "print(f\"    Of which {N_TRAIN} are used to train the network\")\n",
    "print(f\"    Of which {N_VAL} are used to validate the network\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"    Resulting in {STEPS_PER_EPOCH} steps per epoch for training\")\n",
    "print(f\"    Resulting in {VALIDATION_STEPS} steps per epoch for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.0001,\n",
    "  decay_steps=STEPS_PER_EPOCH * 10,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = HDF5Generator(f_train, \"traces\", \"expected\", \"data\", indices=None, batch_size=BATCH_SIZE)\n",
    "validation_generator = HDF5Generator(f_val, \"traces\", \"expected\", \"data\", indices=None, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(2000, dtype=\"float64\"),\n",
    "    tf.keras.layers.Dense(500, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(200, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['binary_crossentropy', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_checkpoint = f\"{directory_data}/mlp_checkpoints-detectcore/saved-model-{{epoch:03d}}-{{val_accuracy:.5f}}.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath_checkpoint, monitor='val_accuracy', verbose=1, save_best_only=False, mode='max')\n",
    "callbacks_list = [checkpoint, tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=20, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start the actual training.\n",
    "Note that we used early stopping with patience 20. To get the best model, retrieve the model 20 epochs before the last model from the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x = training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    validation_steps = VALIDATION_STEPS,\n",
    "    epochs=1000,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network -- predict bit\n",
    "\n",
    "We now define the MLP network that we will use for predict the encoded bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_train = f\"{directory_data}/mlp-predictbit-training-data.hdf5\"\n",
    "fname_val = f\"{directory_data}/mlp-predictbit-validation-data.hdf5\"\n",
    "    \n",
    "f_train = h5py.File(fname_train, 'r', libver='latest')\n",
    "f_val = h5py.File(fname_val, 'r', libver='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = f_train[\"data\"][\"traces\"].shape[0]\n",
    "N_VAL = f_val[\"data\"][\"traces\"].shape[0]\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\n",
    "VALIDATION_STEPS = N_VAL // BATCH_SIZE\n",
    "\n",
    "print(f\"Summary:\")\n",
    "print(f\"  Total number of traces: {N_TRAIN + N_VAL}\")\n",
    "print(f\"    Of which {N_TRAIN} are used to train the network\")\n",
    "print(f\"    Of which {N_VAL} are used to validate the network\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"    Resulting in {STEPS_PER_EPOCH} steps per epoch for training\")\n",
    "print(f\"    Resulting in {VALIDATION_STEPS} steps per epoch for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "  0.0001,\n",
    "  decay_steps=STEPS_PER_EPOCH * 10,\n",
    "  decay_rate=1,\n",
    "  staircase=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = HDF5Generator(f_train, \"traces\", \"expected\", \"data\", indices=None, batch_size=BATCH_SIZE)\n",
    "validation_generator = HDF5Generator(f_val, \"traces\", \"expected\", \"data\", indices=None, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(1000, dtype=\"float64\"),\n",
    "    tf.keras.layers.Dense(500, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(200, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(100, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['binary_crossentropy', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_checkpoint = f\"{directory_data}/mlp_checkpoints-predictbit/saved-model-{{epoch:03d}}-{{val_accuracy:.5f}}.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath_checkpoint, monitor='val_accuracy', verbose=1, save_best_only=False, mode='max')\n",
    "callbacks_list = [checkpoint, tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=20, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start the actual training.\n",
    "Note that we used early stopping with patience 20. To get the best model, retrieve the model 20 epochs before the last model from the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x = training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    validation_steps = VALIDATION_STEPS,\n",
    "    epochs=1000,\n",
    "    callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('letsgetphysical')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ad583c4c013fce90e8ce0308c17365f7d912fe91c0aacff7e52a3a38123cd99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
